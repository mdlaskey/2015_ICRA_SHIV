\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bicchi2000robotic}
\citation{argall2009survey}
\citation{abbeel2007application}
\citation{abbeel2008apprenticeship}
\citation{van2010superhuman}
\citation{pinto2015supersizing}
\citation{grollman2007dogged}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{NIPS2014_5421}
\citation{duvallet2013imitation}
\citation{ross2013learning}
\citation{ross2010reduction}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{ross2013learning}
\citation{duvallet2013imitation}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Three roll-outs on a Zymark 3-DOF Robot of a fully trained grasping in clutter policy (one per column, top to bottom) which was trained using a hierarchy of three supervisors consisting of an analytical motion planning, crowd-sourcing and human expert. Red shapes indicate clutter objects and the robot is trained to reach the yellow circle. The trained manipulation policy is represented as a deep neural network that recieves as input an image of the scene and outputs a change in state position. The resulting policy learns to sweep away clutter objects and reach the goal object. \relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}{Three roll-outs on a Zymark 3-DOF Robot of a fully trained grasping in clutter policy (one per column, top to bottom) which was trained using a hierarchy of three supervisors consisting of an analytical motion planning, crowd-sourcing and human expert. Red shapes indicate clutter objects and the robot is trained to reach the yellow circle. The trained manipulation policy is represented as a deep neural network that recieves as input an image of the scene and outputs a change in state position. The resulting policy learns to sweep away clutter objects and reach the goal object. \relax }{figure.caption.1}{}}
\citation{bicchi2000robotic}
\citation{katz2008can}
\citation{pinto2015supersizing}
\citation{nieuwenhuisen2013mobile}
\citation{mason1986mechanics}
\citation{cosgun2011push}
\citation{kingnonprehensile}
\citation{kitaevphysics}
\citation{leeper2012strategies}
\citation{levine2015end}
\citation{ross2013learning}
\citation{kim2013maximum}
\citation{duvallet2013imitation}
\citation{laskeyshiv}
\citation{chernova2009interactive}
\citation{judah2011active}
\citation{grollman2007dogged}
\citation{kim2013maximum}
\citation{laskeyshiv}
\citation{bengio2009curriculum}
\citation{sanger1994neural}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Problem Statement}{2}{section.3}}
\citation{ross2010reduction}
\citation{levine2015end}
\citation{kitaevphysics}
\citation{ross2010reduction}
\citation{ross2010reduction}
\citation{scholkopf2002learning}
\newlabel{eq:LFD_obj}{{1}{3}{Problem Statement}{equation.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Approach and Background}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Details on DAgger: Dataset Aggregation}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {IV-A.1}Step 1}{3}{subsubsection.4.1.1}}
\newlabel{eq:super_objj}{{2}{3}{Step 1}{equation.4.2}{}}
\citation{NIPS2014_5421}
\citation{ross2010reduction}
\citation{sutton1998reinforcement}
\citation{kitaevphysics}
\citation{kingnonprehensile}
\citation{opencv_library}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {IV-A.2}Step 2}{4}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}DAgger with Supervisor Hierarchy}{4}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Robot Grasping in Clutter}{4}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The interface AMT workers see for providing feedback to the robot for the grasping in clutter task. The pink overlay indicates the desire change in the robot position with respect to the current robot state. Then AMT workers can use their intuition for how objects respond to force to provide examples of how the robot should behave. On the left side is an example of a robot state and on the right side is an example a human supervisor would provide via a Computer Mouse.\relax }}{4}{figure.caption.2}}
\newlabel{fig:overlays}{{2}{4}{\footnotesize The interface AMT workers see for providing feedback to the robot for the grasping in clutter task. The pink overlay indicates the desire change in the robot position with respect to the current robot state. Then AMT workers can use their intuition for how objects respond to force to provide examples of how the robot should behave. On the left side is an example of a robot state and on the right side is an example a human supervisor would provide via a Computer Mouse.\relax }{figure.caption.2}{}}
\citation{tensorflow2015-whitepaper}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  Example of how the binary mask applied to each RGB channel affects the input data. As you can see it reduces the dimensionality of each channel's pixel value from the range of [0,255] to [0,1], which can allow for less data needed during learning and robustness to small lighting conditions. \relax }}{5}{figure.caption.3}}
\newlabel{fig:b_seg}{{3}{5}{\footnotesize Example of how the binary mask applied to each RGB channel affects the input data. As you can see it reduces the dimensionality of each channel's pixel value from the range of [0,255] to [0,1], which can allow for less data needed during learning and robustness to small lighting conditions. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Experiments}{5}{section.6}}
\newlabel{sec:Exp}{{VI}{5}{Experiments}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-A}Experimental Setup}{5}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  Shown above is a Zymark robot. The robot consists of an 3DOF arm that lies in a planar workspace. It is able to extend its arm and rotate about a fixed base. The robot can also open and close its gripper.\relax }}{5}{figure.caption.4}}
\newlabel{fig:robot}{{4}{5}{\footnotesize Shown above is a Zymark robot. The robot consists of an 3DOF arm that lies in a planar workspace. It is able to extend its arm and rotate about a fixed base. The robot can also open and close its gripper.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The set of objects that the robot was trained on. The Training objects on the right are the four objects used in training. The test objects on the left represent represent objects that were found in our test configurations. The test objects vary in size and shape from the training objects, which can test how well the robot learns to manipulate unknown objects. Every test configuration contained at least one object from this set to guarantee it wasn't trained on. \relax }}{5}{figure.caption.5}}
\newlabel{fig:shape_set}{{5}{5}{\footnotesize The set of objects that the robot was trained on. The Training objects on the right are the four objects used in training. The test objects on the left represent represent objects that were found in our test configurations. The test objects vary in size and shape from the training objects, which can test how well the robot learns to manipulate unknown objects. Every test configuration contained at least one object from this set to guarantee it wasn't trained on. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  On the right is examples of training configurations we presented to the robot. The shapes were arranged around the goal object in different poses and in different parts of the workspace. At testing time, we had similar configurations but some shapes were unknown objects that were not trained on. \relax }}{6}{figure.caption.6}}
\newlabel{fig:suc_meas}{{6}{6}{\footnotesize On the right is examples of training configurations we presented to the robot. The shapes were arranged around the goal object in different poses and in different parts of the workspace. At testing time, we had similar configurations but some shapes were unknown objects that were not trained on. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-B}Hierarchical Supervisors}{6}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-C}Quality of Crowdsourced Supervisor}{6}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-D}Advancing in the Hierarchy}{6}{subsection.6.4}}
\citation{scholkopf2002learning}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  The Score (or \% Succesful) and cost of each policy trained with a supervisor is reported. The bar graphs shows how many successful and failed situations occurred. The top row corresponds to the experiments on hierarchical supervisors: Algorithmic, Human Expert and the hierarchy of Algorithmic to Human Expert. The bottom corresponds to experiments on testing a crowdsourced supervisor: Crowdsourced and the hierarchy Algorithmic and Crowdsourced, AS and Human Expert, AS, Crowdsourced and Human Expert. \relax }}{7}{figure.caption.7}}
\newlabel{fig:perf_results}{{7}{7}{\footnotesize The Score (or \% Succesful) and cost of each policy trained with a supervisor is reported. The bar graphs shows how many successful and failed situations occurred. The top row corresponds to the experiments on hierarchical supervisors: Algorithmic, Human Expert and the hierarchy of Algorithmic to Human Expert. The bottom corresponds to experiments on testing a crowdsourced supervisor: Crowdsourced and the hierarchy Algorithmic and Crowdsourced, AS and Human Expert, AS, Crowdsourced and Human Expert. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-E}Scaling the Hierarchy}{7}{subsection.6.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  The performance and cost of the policy trained on the full hierarchy with of Algorithmic, Crowdsourced and Human Expert. The policy achieves a success rate or score of $80\%$ success. The total cost though after 320 trials is only 113. \relax }}{7}{figure.caption.8}}
\newlabel{fig:big_data}{{8}{7}{\footnotesize The performance and cost of the policy trained on the full hierarchy with of Algorithmic, Crowdsourced and Human Expert. The policy achieves a success rate or score of $80\%$ success. The total cost though after 320 trials is only 113. \relax }{figure.caption.8}{}}
\bibstyle{IEEEtranS}
\bibdata{references}
\bibcite{tensorflow2015-whitepaper}{1}
\bibcite{abbeel2007application}{2}
\bibcite{abbeel2008apprenticeship}{3}
\bibcite{argall2009survey}{4}
\bibcite{bengio2009curriculum}{5}
\bibcite{bicchi2000robotic}{6}
\bibcite{chernova2009interactive}{7}
\bibcite{cosgun2011push}{8}
\bibcite{duvallet2013imitation}{9}
\bibcite{grollman2007dogged}{10}
\bibcite{NIPS2014_5421}{11}
\bibcite{judah2011active}{12}
\bibcite{katz2008can}{13}
\bibcite{kim2013maximum}{14}
\bibcite{kingnonprehensile}{15}
\bibcite{kitaevphysics}{16}
\bibcite{laskeyshiv}{17}
\bibcite{leeper2012strategies}{18}
\bibcite{levine2015end}{19}
\bibcite{mason1986mechanics}{20}
\bibcite{nieuwenhuisen2013mobile}{21}
\bibcite{pinto2015supersizing}{22}
\bibcite{ross2010efficient}{23}
\bibcite{ross2010reduction}{24}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The four failure modes in our test set on the fully trained policy. On the left the robot sweeps to far and the collision objects jam up against the edge of the work space. On the right, both the banana and the boot are prone to get entrapped in the gripper during pushing and prevent the gripper from reaching the circle. \relax }}{8}{figure.caption.9}}
\newlabel{fig:failure_modes}{{9}{8}{\footnotesize The four failure modes in our test set on the fully trained policy. On the left the robot sweeps to far and the collision objects jam up against the edge of the work space. On the right, both the banana and the boot are prone to get entrapped in the gripper during pushing and prevent the gripper from reaching the circle. \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The performance of each policy trained with a different data management strategy is reported. The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. Each policy was trained with the AS to Human Expert hierarchical supervisor. From left to right is the following data management strategies: Dataset Aggregation, Weight Transfer and Regularization. Regularization achieves the highest performing score of (0.65) \relax }}{8}{figure.caption.10}}
\newlabel{fig:cost_result}{{10}{8}{\footnotesize The performance of each policy trained with a different data management strategy is reported. The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. Each policy was trained with the AS to Human Expert hierarchical supervisor. From left to right is the following data management strategies: Dataset Aggregation, Weight Transfer and Regularization. Regularization achieves the highest performing score of (0.65) \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Discussions and Future Work}{8}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Acknowledgments}{8}{section.8}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.11}}
\bibcite{ross2013learning}{25}
\bibcite{scholkopf2002learning}{26}
\bibcite{sutton1998reinforcement}{27}
\bibcite{van2010superhuman}{28}
