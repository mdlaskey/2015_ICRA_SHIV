\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bicchi2000robotic}
\citation{argall2009survey}
\citation{abbeel2007application}
\citation{abbeel2008apprenticeship}
\citation{van2010superhuman}
\citation{pinto2015supersizing}
\citation{grollman2007dogged}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{NIPS2014_5421}
\citation{duvallet2013imitation}
\citation{ross2013learning}
\citation{ross2010reduction}
\citation{ross2010efficient}
\citation{ross2010reduction}
\citation{ross2013learning}
\citation{duvallet2013imitation}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Three roll-outs on a Zymark 3-DOF Robot of a fully trained grasping in clutter policy (one per column, bottom to top) which was trained using a hierarchy of three supervisors consisting of an analytical motion planning, crowd-sourcing and human expert. Red shapes indicate clutter objects and the robot is trained to reach the yellow circle. The trained manipulation policy is represented as a deep neural network that recieves as input an image of the scene and outputs a change in state position. The resulting policy learns to sweep away clutter objects and reach the goal object. \relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}{Three roll-outs on a Zymark 3-DOF Robot of a fully trained grasping in clutter policy (one per column, bottom to top) which was trained using a hierarchy of three supervisors consisting of an analytical motion planning, crowd-sourcing and human expert. Red shapes indicate clutter objects and the robot is trained to reach the yellow circle. The trained manipulation policy is represented as a deep neural network that recieves as input an image of the scene and outputs a change in state position. The resulting policy learns to sweep away clutter objects and reach the goal object. \relax }{figure.caption.1}{}}
\citation{bicchi2000robotic}
\citation{katz2008can}
\citation{pinto2015supersizing}
\citation{nieuwenhuisen2013mobile}
\citation{mason1986mechanics}
\citation{cosgun2011push}
\citation{kingnonprehensile}
\citation{kitaevphysics}
\citation{leeper2012strategies}
\citation{levine2015end}
\citation{ross2013learning}
\citation{kim2013maximum}
\citation{duvallet2013imitation}
\citation{laskeyshiv}
\citation{chernova2009interactive}
\citation{judah2011active}
\citation{grollman2007dogged}
\citation{kim2013maximum}
\citation{laskeyshiv}
\citation{bengio2009curriculum}
\citation{sanger1994neural}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Problem Statement}{2}{section.3}}
\citation{ross2010reduction}
\citation{levine2015end}
\citation{kitaevphysics}
\citation{ross2010reduction}
\citation{ross2010reduction}
\citation{scholkopf2002learning}
\newlabel{eq:LFD_obj}{{1}{3}{Problem Statement}{equation.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Approach and Background}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Details on DAgger: Dataset Aggregation}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {IV-A.1}Step 1}{3}{subsubsection.4.1.1}}
\newlabel{eq:super_objj}{{2}{3}{Step 1}{equation.4.2}{}}
\citation{NIPS2014_5421}
\citation{ross2010reduction}
\citation{sutton1998reinforcement}
\citation{kitaevphysics}
\citation{kingnonprehensile}
\citation{opencv_library}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {IV-A.2}Step 2}{4}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}DAgger with Supervisor Hierarchy}{4}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Robot Grasping in Clutter}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Grasping in Clutter}{4}{subsection.5.1}}
\newlabel{sec:task}{{V-A}{4}{Grasping in Clutter}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Supervisor Hierarchy}{4}{subsection.5.2}}
\newlabel{sec:hier}{{V-B}{4}{Supervisor Hierarchy}{subsection.5.2}{}}
\citation{tensorflow2015-whitepaper}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The interface AMT workers see for providing feedback to the robot for the grasping in clutter task. The pink overlay indicates the desire change in the robot position with respect to the current robot state. Then AMT workers can use their intuition for how objects respond to force to provide examples of how the robot should behave. On the left side is an example of a robot state and on the right side is an example a human supervisor would provide via a Computer Mouse.\relax }}{5}{figure.caption.2}}
\newlabel{fig:overlays}{{2}{5}{\footnotesize The interface AMT workers see for providing feedback to the robot for the grasping in clutter task. The pink overlay indicates the desire change in the robot position with respect to the current robot state. Then AMT workers can use their intuition for how objects respond to force to provide examples of how the robot should behave. On the left side is an example of a robot state and on the right side is an example a human supervisor would provide via a Computer Mouse.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  Shown above is a Zymark robot. The robot consists of an 3DOF arm that lies in a planar workspace. It is able to extend its arm and rotate about a fixed base. The robot can also open and close its gripper.\relax }}{5}{figure.caption.3}}
\newlabel{fig:robot}{{3}{5}{\footnotesize Shown above is a Zymark robot. The robot consists of an 3DOF arm that lies in a planar workspace. It is able to extend its arm and rotate about a fixed base. The robot can also open and close its gripper.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Neural Network Policy Architecture}{5}{subsection.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Experiments}{5}{section.6}}
\newlabel{sec:Exp}{{VI}{5}{Experiments}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The set of objects that the robot was trained on. The Training objects on the right are the four objects used in training. The test objects on the left represent represent objects that were found in our test configurations. The test objects vary in size and shape from the training objects, which can test how well the robot learns to manipulate unknown objects. Every test configuration contained at least one object from this set to guarantee it wasn't trained on. \relax }}{6}{figure.caption.4}}
\newlabel{fig:shape_set}{{4}{6}{\footnotesize The set of objects that the robot was trained on. The Training objects on the right are the four objects used in training. The test objects on the left represent represent objects that were found in our test configurations. The test objects vary in size and shape from the training objects, which can test how well the robot learns to manipulate unknown objects. Every test configuration contained at least one object from this set to guarantee it wasn't trained on. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The different measures of performance used in our scoring metric for how well the robot performed. Not Contact corresponds to when the gripper is never in contact with yellow circle. Contact corresponds to when the robot ends with the gripper in contact with yellow circle. Grasped is defined as when the yellow circle is inside the gripper. \relax }}{6}{figure.caption.5}}
\newlabel{fig:suc_meas}{{5}{6}{\footnotesize The different measures of performance used in our scoring metric for how well the robot performed. Not Contact corresponds to when the gripper is never in contact with yellow circle. Contact corresponds to when the robot ends with the gripper in contact with yellow circle. Grasped is defined as when the yellow circle is inside the gripper. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-A}Experimental Setup}{6}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-B}Hierarchical Supervisors}{6}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-C}Quality of Crowdsourced Supervisor}{6}{subsection.6.3}}
\citation{scholkopf2002learning}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  The performance and cost of each policy trained with a supervisor is reported. The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. The top row corresponds to policies only trained with one supervisor: MPIO, Crowdsourced or Human Expert. The bottom corresponds to policies trained with a hierarchy of supervisors: MPIO and Crowdsource, MPIO and Human Expert, MPIO, Crowdsourced and Human Expert. The final bottom right plot demonstrates the full hierarchy, which achieves the best performance (0.8). (WILL CLEAN UP ONCE WE DESCRIBE SCORE) \relax }}{7}{figure.caption.6}}
\newlabel{fig:perf_results}{{6}{7}{\footnotesize The performance and cost of each policy trained with a supervisor is reported. The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. The top row corresponds to policies only trained with one supervisor: MPIO, Crowdsourced or Human Expert. The bottom corresponds to policies trained with a hierarchy of supervisors: MPIO and Crowdsource, MPIO and Human Expert, MPIO, Crowdsourced and Human Expert. The final bottom right plot demonstrates the full hierarchy, which achieves the best performance (0.8). (WILL CLEAN UP ONCE WE DESCRIBE SCORE) \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {8}{9pt}\selectfont  The performance of each policy trained with a different data management strategy is reported. The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. Each policy was trained with the MPIO to Human Expert hierarchical supervisor. From left to right is the following data management strategies: Dataset Aggregation, Weight Transfer and Regularization. Regularization achieves the highest performing score of (0.18) \relax }}{7}{figure.caption.7}}
\newlabel{fig:cost_result}{{7}{7}{\footnotesize The performance of each policy trained with a different data management strategy is reported. The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. Each policy was trained with the MPIO to Human Expert hierarchical supervisor. From left to right is the following data management strategies: Dataset Aggregation, Weight Transfer and Regularization. Regularization achieves the highest performing score of (0.18) \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-D}Advancing in the Hierarchy}{7}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  The performance and cost of right the full hierarchy, which achieves the best performance (0.8). The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. (WILL CLEAN UP ONCE WE DESCRIBE SCORE) \relax }}{7}{figure.caption.8}}
\newlabel{fig:big_data}{{8}{7}{\footnotesize The performance and cost of right the full hierarchy, which achieves the best performance (0.8). The bar graphs shows the breakdown in terms of situations the policy encountered on the test set. (WILL CLEAN UP ONCE WE DESCRIBE SCORE) \relax }{figure.caption.8}{}}
\bibstyle{IEEEtranS}
\bibdata{references}
\bibcite{tensorflow2015-whitepaper}{1}
\bibcite{abbeel2007application}{2}
\bibcite{abbeel2008apprenticeship}{3}
\bibcite{argall2009survey}{4}
\bibcite{bengio2009curriculum}{5}
\bibcite{bicchi2000robotic}{6}
\bibcite{chernova2009interactive}{7}
\bibcite{duvallet2013imitation}{8}
\bibcite{grollman2007dogged}{9}
\bibcite{NIPS2014_5421}{10}
\bibcite{judah2011active}{11}
\bibcite{katz2008can}{12}
\bibcite{kim2013maximum}{13}
\bibcite{kingnonprehensile}{14}
\bibcite{kitaevphysics}{15}
\bibcite{levine2015end}{16}
\bibcite{li2010contextual}{17}
\bibcite{nieuwenhuisen2013mobile}{18}
\bibcite{pinto2015supersizing}{19}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-E}Scaling the Hierarchy}{8}{subsection.6.5}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Discussions and Future Work}{8}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Acknowledgments}{8}{section.8}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.9}}
\bibcite{ross2010efficient}{20}
\bibcite{ross2010reduction}{21}
\bibcite{ross2013learning}{22}
\bibcite{scholkopf2002learning}{23}
\bibcite{sutton1998reinforcement}{24}
\bibcite{van2010superhuman}{25}
