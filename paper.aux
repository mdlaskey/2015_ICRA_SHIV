\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ferrari1992}
\citation{kim2012physically}
\citation{weisz2012pose}
\citation{kehoe2012estimating}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\relax \fontsize  {10}{12.00pt}\selectfont  \color  {red} \textsc  {\MakeLowercase  {[todo:\nobreakspace  {}New teaser photo, not sure if we should have some sort of real object or not]}}}(Top Left) Image of a common household measuring cup. (Top Right) Image from a PR2 Primesense camera that reflects the uncertainty that can be induced in objects from transparency. (Bottom) Comparision of Multi-Armed Bandit Techniques (Bayes UCB, Thompson, Gittins) vs. Monte-Carlo sampling to determine the best grasp in a set of 1000 grasps. As you can see the bandit techniques converge in simple regret Eq. \ref  {eq:simple_regret} a magnitude faster than the traditional approach of Monte-Carlo sampling to determine the highest quality grasp. \relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rot_shapes}{{1}{1}{\todo {New teaser photo, not sure if we should have some sort of real object or not}(Top Left) Image of a common household measuring cup. (Top Right) Image from a PR2 Primesense camera that reflects the uncertainty that can be induced in objects from transparency. (Bottom) Comparision of Multi-Armed Bandit Techniques (Bayes UCB, Thompson, Gittins) vs. Monte-Carlo sampling to determine the best grasp in a set of 1000 grasps. As you can see the bandit techniques converge in simple regret Eq. \ref {eq:simple_regret} a magnitude faster than the traditional approach of Monte-Carlo sampling to determine the highest quality grasp. \relax }{figure.caption.1}{}}
\citation{barto1998reinforcement}
\citation{lai1985asymptotically}
\citation{robbins1952some}
\citation{simon1989optimal}
\citation{rothschild1974two}
\citation{st2012online}
\citation{madani2004budgeted}
\citation{christopoulos2007handling}
\citation{kehoe2012toward}
\citation{dragiev2011}
\citation{hollinger2013}
\citation{goldberg1990bayesian}
\citation{stulp2011learning}
\citation{zheng2005}
\citation{christopoulos2007handling}
\citation{weisz2012pose}
\citation{kim2012physically}
\citation{christopoulos2007handling}
\citation{kehoe2012estimating}
\citation{kehoe2012toward}
\citation{hsiao2011bayesian}
\citation{christopoulos2007handling}
\citation{kehoe2012estimating}
\citation{kehoe2012toward}
\citation{caflisch1998monte}
\citation{kehoe2012estimating}
\citation{laaksonen2012probabilistic}
\citation{andrieu2003introduction}
\citation{rasmussen2006}
\citation{williams2007}
\citation{dragiev2011}
\citation{hollinger2013}
\citation{dragiev2011}
\citation{mahler2015gp}
\citation{ferrari1992}
\citation{miller2004graspit}
\citation{73}
\citation{vahrenkamp2010simo}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Preliminaries and Problem Definition}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Grasp Metric}{2}{subsection.3.1}}
\citation{pokorny2013classical}
\citation{christopoulos2007handling}
\citation{kehoe2012toward}
\citation{christopoulos2007handling}
\citation{mahler}
\citation{barfoot2014Pose}
\citation{kehoe2012estimating}
\citation{mooring1986determination}
\citation{zheng2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Line of action}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Types of Uncertainty}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-C}1}Distribution on Shape}{3}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-C}2}Distribution on Pose}{3}{subsubsection.3.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-C}3}Distribution on Motion}{3}{subsubsection.3.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-C}4}Distribution on Friction Coefficient}{3}{subsubsection.3.3.4}}
\citation{mahler2015gp}
\citation{robbins1985some}
\citation{simon1989optimal}
\citation{rothschild1974two}
\citation{st2012online}
\citation{bubeck2009pure}
\citation{robbins1985some}
\citation{bergemann2006bandit}
\citation{lai1985asymptotically}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A graphical model that illustrates the relationship between the different types of uncertainty in an object. Center of Mass uncertainty is dependent on the pose and shape of the object, however friction coefficient is independent of all other types. {\relax \fontsize  {10}{12.00pt}\selectfont  \color  {red} \textsc  {\MakeLowercase  {[todo:\nobreakspace  {}Should we use words or symbols for the graphical model?]}}}\relax }}{4}{figure.caption.2}}
\newlabel{fig:graphical_model}{{2}{4}{A graphical model that illustrates the relationship between the different types of uncertainty in an object. Center of Mass uncertainty is dependent on the pose and shape of the object, however friction coefficient is independent of all other types. \todo {Should we use words or symbols for the graphical model?}\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of a grasp plan $\Gamma $ composed of two lines of action, $\gamma _1(t)$ and $\gamma _2(t)$\relax }}{4}{figure.caption.3}}
\newlabel{fig:line_of_action}{{3}{4}{Illustration of a grasp plan $\Gamma $ composed of two lines of action, $\gamma _1(t)$ and $\gamma _2(t)$\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Problem Definition}{4}{subsection.3.4}}
\newlabel{eq:problem_def}{{1}{4}{Problem Definition}{equation.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Multi-Armed Bandits for Grasp Selection}{4}{section.4}}
\newlabel{eq:simple_regret}{{2}{4}{Multi-Armed Bandits for Grasp Selection}{equation.4.2}{}}
\citation{maron1993hoeffding}
\citation{mnih2008empirical}
\citation{gabillon2012best}
\citation{audibert2010best}
\citation{gabillon2012best}
\citation{bubeck2009pure}
\citation{kaufmann2012bayesian}
\citation{agrawal2011analysis}
\citation{chapelle2011empirical}
\citation{chapelle2011empirical}
\citation{kaufmann2012bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Fixed Confidence}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Fixed Budget}{5}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Bandit Algorithms For Best Arm Identification}{5}{section.5}}
\newlabel{sec:bandit_algorithm}{{V}{5}{Bandit Algorithms For Best Arm Identification}{section.5}{}}
\newlabel{eq:shape_sampling}{{3}{5}{Bandit Algorithms For Best Arm Identification}{equation.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A graphical model that illustrates the relationship between the Bernoulli distribution of the probability force closure and its conjugate prior Beta distribution that has two shape parameters $\alpha $ and $\beta $ \relax }}{5}{figure.caption.4}}
\newlabel{fig:beta_model}{{4}{5}{A graphical model that illustrates the relationship between the Bernoulli distribution of the probability force closure and its conjugate prior Beta distribution that has two shape parameters $\alpha $ and $\beta $ \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Thompson Sampling}{5}{subsection.5.1}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Thompson Sampling for Beta-Bernoulli Process\relax }}{5}{algocf.1}}
\citation{katehakis1987multi}
\citation{bubeck2009pure}
\citation{brown}
\citation{brown}
\citation{brown}
\citation{best_arm}
\citation{brown}
\citation{best_arm}
\citation{gabillon2012bes}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Bayes-UCB}{6}{subsection.5.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Bayes-UCB for Beta-Bernoulli Process\relax }}{6}{algocf.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}The Gittins Index Method}{6}{subsection.5.3}}
\newlabel{eq:git_indices}{{\unhbox \voidb@x \hbox {V-C}}{6}{The Gittins Index Method}{subsection.5.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces The Gittins Index Method for Beta-Bernoulli Process\relax }}{6}{algocf.3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Experiments}{6}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-A}}Multi-Armed Bandit Experiments}{6}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-B}}Sensitivity Analysis }{6}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Comparison of Simple Regret convergence for the four sequential decision methods (Monte-Carlo, Bayes - UCB, Thompson, Gittins). Graph is averaged over 100 shapes from the Brown Sillohoute Dataset \cite  {brown} with a set $|G|=1000$ for each shape. As you can see the BMAB methods converge almost a magnitude faster than random allocation. It is worth noting that Gittins outperform the other two algorithms, which is useful when choosing which one to implement \relax }}{7}{figure.caption.8}}
\newlabel{fig:simple_regret}{{5}{7}{\footnotesize Comparison of Simple Regret convergence for the four sequential decision methods (Monte-Carlo, Bayes - UCB, Thompson, Gittins). Graph is averaged over 100 shapes from the Brown Sillohoute Dataset \cite {brown} with a set $|G|=1000$ for each shape. As you can see the BMAB methods converge almost a magnitude faster than random allocation. It is worth noting that Gittins outperform the other two algorithms, which is useful when choosing which one to implement \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Comparison of sample per grasp for the four sequential decision methods (Random, Bayes - UCB, Thompson, Gittins). Graph is averaged over 100 shapes from the Brown Silhouette Dataset \cite  {brown} with a set $|G|=1000$ for each shape. The best grasps are ranked 1 and worst are 1000. As you can see the MAB algorithm intelligently allocate samples towards high quality grasps based on past observations, where Monte-Carlo Integration takes a uniform approach to allocation. \cite  {best_arm}\relax }}{7}{figure.caption.9}}
\newlabel{fig:pulls_per_grasp}{{6}{7}{\footnotesize Comparison of sample per grasp for the four sequential decision methods (Random, Bayes - UCB, Thompson, Gittins). Graph is averaged over 100 shapes from the Brown Silhouette Dataset \cite {brown} with a set $|G|=1000$ for each shape. The best grasps are ranked 1 and worst are 1000. As you can see the MAB algorithm intelligently allocate samples towards high quality grasps based on past observations, where Monte-Carlo Integration takes a uniform approach to allocation. \cite {best_arm}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Limitations}{7}{section.7}}
\citation{maron1993hoeffding}
\citation{mnih2008empirical}
\citation{audibert2010best}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Sensitivity Analysis for Thompson Sampling and the Gittens Index Method under coefficient of friction uncertainty $\sigma _{fric} = \delimiter "4266308 0.05,0.2, 0.4 \delimiter "5267309 $ radians from top to bottom on a 40 x 40 unit workspace averaged over 10 shapes from the Brown Vision Lab Data set. The increase in noise has little effect on the convergence of the two algorithms in simple regret.\relax }}{8}{figure.caption.10}}
\newlabel{fig:fric_sens}{{7}{8}{\footnotesize Sensitivity Analysis for Thompson Sampling and the Gittens Index Method under coefficient of friction uncertainty $\sigma _{fric} = \lbrace 0.05,0.2, 0.4 \rbrace $ radians from top to bottom on a 40 x 40 unit workspace averaged over 10 shapes from the Brown Vision Lab Data set. The increase in noise has little effect on the convergence of the two algorithms in simple regret.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Sensitivity Analysis for Thompson Sampling under rotation uncertainty $\sigma _{rot}$ in the range of $(0.03, 0.24)$ radians from top to bottom on a 40 x 40 unit workspace averaged over 10 shapes from the Brown Vision Lab Data set. As you can see the increase in noise effects performance, however the 5500 samples needed for convergence at the the highest level of noise (which is over half the workspace) is much less that the samples needed for uniform allocation to converge in Fig. \ref  {fig:simple_regret}\relax }}{8}{figure.caption.11}}
\newlabel{fig:rot_sens}{{8}{8}{\footnotesize Sensitivity Analysis for Thompson Sampling under rotation uncertainty $\sigma _{rot}$ in the range of $(0.03, 0.24)$ radians from top to bottom on a 40 x 40 unit workspace averaged over 10 shapes from the Brown Vision Lab Data set. As you can see the increase in noise effects performance, however the 5500 samples needed for convergence at the the highest level of noise (which is over half the workspace) is much less that the samples needed for uniform allocation to converge in Fig. \ref {fig:simple_regret}\relax }{figure.caption.11}{}}
\citation{ferrari1992}
\citation{kim2012physically}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Two shapes shown from the Brown Visual Lab Dataset,from Left to Right the variance on rotation $\sigma _{rot}$ is increased from $2^{\qopname  \relax o{deg}}$ to $13^{\qopname  \relax o{deg}}$. As you can see the overall variance increase effects the quality of the top grasp in the set of possible grasps. Furthermore for Shape 2, the grasp with low rotational variance is different than that for higher variance because the original grasp is more likely to touch the area of higher shape uncertainty when subjected to high variance in rotation. \relax }}{9}{figure.caption.12}}
\newlabel{fig:rot_shapes}{{9}{9}{Two shapes shown from the Brown Visual Lab Dataset,from Left to Right the variance on rotation $\sigma _{rot}$ is increased from $2^{\deg }$ to $13^{\deg }$. As you can see the overall variance increase effects the quality of the top grasp in the set of possible grasps. Furthermore for Shape 2, the grasp with low rotational variance is different than that for higher variance because the original grasp is more likely to touch the area of higher shape uncertainty when subjected to high variance in rotation. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {lTop Grasps Shown for Shape 1}}}{9}{subfigure.9.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Top Grasps Shown for Shape 2}}}{9}{subfigure.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Sensitivity Analysis for Thompson Sampling under translation uncertainty $\sigma _{trans} = \delimiter "4266308 3, 12, 24 \delimiter "5267309 $ units from top to bottom on a 40 x 40 unit workspace. As you can see the increase in noise effects performance, however the 5000 samples needed for Gittins to converge at the the highest level of noise (which is a variance of over half the workspace) is much less that the samples needed for uniform allocation to converge in Fig. \ref  {fig:simple_regret}\relax }}{9}{figure.caption.13}}
\newlabel{fig:trans_sens}{{10}{9}{\footnotesize Sensitivity Analysis for Thompson Sampling under translation uncertainty $\sigma _{trans} = \lbrace 3, 12, 24 \rbrace $ units from top to bottom on a 40 x 40 unit workspace. As you can see the increase in noise effects performance, however the 5000 samples needed for Gittins to converge at the the highest level of noise (which is a variance of over half the workspace) is much less that the samples needed for uniform allocation to converge in Fig. \ref {fig:simple_regret}\relax }{figure.caption.13}{}}
\citation{kehoe2012estimating}
\citation{christopoulos2007handling}
\citation{73}
\citation{miller2004graspit}
\citation{bergemann2006bandit}
\bibstyle{IEEEtranS}
\bibdata{references}
\bibcite{agrawal2011analysis}{1}
\bibcite{andrieu2003introduction}{2}
\bibcite{audibert2010best}{3}
\bibcite{barfoot2014Pose}{4}
\bibcite{barto1998reinforcement}{5}
\bibcite{bergemann2006bandit}{6}
\bibcite{bubeck2009pure}{7}
\bibcite{caflisch1998monte}{8}
\bibcite{chapelle2011empirical}{9}
\bibcite{christopoulos2007handling}{10}
\bibcite{dragiev2011}{11}
\bibcite{ferrari1992}{12}
\bibcite{gabillon2012best}{13}
\bibcite{goldberg1990bayesian}{14}
\bibcite{hollinger2013}{15}
\bibcite{hsiao2011bayesian}{16}
\bibcite{katehakis1987multi}{17}
\bibcite{kaufmann2012bayesian}{18}
\bibcite{kehoe2012estimating}{19}
\bibcite{kehoe2012toward}{20}
\bibcite{kim2012physically}{21}
\bibcite{laaksonen2012probabilistic}{22}
\bibcite{lai1985asymptotically}{23}
\bibcite{73}{24}
\bibcite{madani2004budgeted}{25}
\bibcite{mahler2015gp}{26}
\bibcite{maron1993hoeffding}{27}
\bibcite{miller2004graspit}{28}
\bibcite{mnih2008empirical}{29}
\bibcite{pokorny2013classical}{30}
\bibcite{rasmussen2006}{31}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Conclusion}{10}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {IX}Future Work}{10}{section.9}}
\@writefile{toc}{\contentsline {section}{References}{10}{section*.14}}
\bibcite{rasmussen2010gaussian}{32}
\bibcite{robbins1985some}{33}
\bibcite{rothschild1974two}{34}
\bibcite{simon1989optimal}{35}
\bibcite{solak2003derivative}{36}
\bibcite{st2012online}{37}
\bibcite{stulp2011learning}{38}
\bibcite{vahrenkamp2010simo}{39}
\bibcite{weisz2012pose}{40}
\bibcite{williams2007}{41}
\bibcite{zheng2005}{42}
\citation{mahler2015opt}
\citation{rasmussen2010gaussian}
\citation{solak2003derivative}
\citation{dragiev2011}
\citation{williams2007}
\citation{ferrari1992}
\@writefile{toc}{\contentsline {section}{Appendix: Gaussian Process Implicit Surface for Representing Shape Uncertainty}{11}{section*.15}}
\newlabel{sec:Appendix}{{A}{11}{\appendixname \nobreakspace \\* Gaussian Process Implicit Surface for Representing Shape Uncertainty}{section*.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Gaussian Process (GP) Background}{11}{subsection.Appendix.A.1}}
\newlabel{sec:GP}{{A}{11}{Gaussian Process (GP) Background}{subsection.Appendix.A.1}{}}
\newlabel{eq:mean_gradient}{{6}{11}{Gaussian Process (GP) Background}{equation.Appendix.A.6}{}}
\newlabel{eq:cov_gradient}{{7}{11}{Gaussian Process (GP) Background}{equation.Appendix.A.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Sampling Shape from GPIS Distribution }{11}{subsection.Appendix.A.2}}
\newlabel{eq:joint_shape}{{8}{11}{Sampling Shape from GPIS Distribution}{equation.Appendix.A.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Shape samples drawn from Eq. \ref  {eq:joint_shape} on the object in the upper left corner. Given a shape sample we highlight the zero-crossing of the level set in black\relax }}{11}{figure.caption.16}}
\newlabel{fig:shape_samples}{{11}{11}{Shape samples drawn from Eq. \ref {eq:joint_shape} on the object in the upper left corner. Given a shape sample we highlight the zero-crossing of the level set in black\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Distribution on Surface Normals}{11}{subsection.Appendix.A.3}}
\newlabel{sec:normals}{{C}{11}{Distribution on Surface Normals}{subsection.Appendix.A.3}{}}
\newlabel{eq:normal_dist}{{9}{11}{Distribution on Surface Normals}{equation.Appendix.A.9}{}}
\citation{christopoulos2007handling}
\citation{mahler2015opt}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Expected Center of Mass}{12}{subsection.Appendix.A.4}}
\newlabel{sec:mass}{{D}{12}{Expected Center of Mass}{subsection.Appendix.A.4}{}}
\newlabel{eq:mass}{{D}{12}{Expected Center of Mass}{subsection.Appendix.A.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  \relax \fontsize  {8}{9pt}\selectfont  Left: A surface with GPIS construction and expected center of mass (black X) Right: The distribution on the density of each point assuming uniform density\relax }}{12}{figure.caption.17}}
\newlabel{fig:GPIS_MASS}{{12}{12}{\footnotesize Left: A surface with GPIS construction and expected center of mass (black X) Right: The distribution on the density of each point assuming uniform density\relax }{figure.caption.17}{}}
